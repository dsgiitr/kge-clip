## ðŸ“Š Results Overview

The `results` folder contains the embeddings we generated using various encoders for both text and images. To gain deeper insights, we applied dimensionality reduction techniques to visualize these embeddings in a more manageable form.

### Embeddings Generated

We used the following encoders to create embeddings for the text and image data:

- **Universal Sentence Encoder (USE)**
- **InferSent**
- **BERT**
- **CLIP**

After generating the embeddings, we applied dimensionality reduction techniques such as `PCA`, `UMAP`, and `T-SNE` to project these high-dimensional embeddings into a 3D space, making them easier to visualize and analyze.

### Dataset Preview

Below is a screenshot of the dataset used for creating these embeddings:

![Dataset Screenshot](path_to_your_screenshot)

### ðŸ”— Kaggle Datasets for Embeddings and Reduced Embeddings

To further explore the embeddings and their reduced forms, we have made the datasets available on Kaggle. You can access them through the following links:

| **Sentence Encoder** | **URL** |
|----------------------|---------|
| Infersent            | [Infersent Dataset](https://kaggle.com/datasets/anantjain1223/infersent-coyo-1k) | 
| Clip-text            | [Clip-text Dataset](https://www.kaggle.com/datasets/anantjain1223/clip-text-coyo-1k) |
| BERT                 | [BERT Dataset](https://www.kaggle.com/datasets/anantjain1223/sentence-transformer-coyo-1k) |
| Universal Sentence Encoder | [USE Dataset](https://www.kaggle.com/datasets/anantjain1223/use-coyo-1k) |

###  Understanding the Results

The embeddings generated by each encoder offer unique perspectives on the text and image data. By reducing their dimensions, we aimed to uncover hidden patterns and relationships that are not immediately visible in higher-dimensional spaces. The visualization of these embeddings provides crucial insights into how well the text and images align contextually and semantically.

**Key Findings:**
- **USE and InferSent** offered more generalized embeddings that captured broader semantic relationships.
- **BERT** embeddings showed finer details, especially in nuanced text-image pairs.
- **CLIP** effectively aligned the text and image embeddings, making it particularly powerful for tasks involving both modalities.

The dimensionally reduced embeddings provide a clearer picture of these relationships, making it easier to understand how each encoder performs and where improvements can be made.

### Conclusion

The results from this project underscore the importance of choosing the right encoder and dimensionality reduction technique based on the specific needs of your task. By comparing different embeddings, we can better understand their strengths and weaknesses, ultimately leading to more effective models in real-world applications.
